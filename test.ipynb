{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers --quiet\n",
    "# !pip install pyaudio\n",
    "# !pip install pytorch\n",
    "# !pip install sounddevice pydub\n",
    "# !pip install gradio\n",
    "# !pip install pyctcdecode\n",
    "# !python -m pip install pypi-kenlm\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import nltk\n",
    "def record_audio(file_name, duration=8, sample_rate=44100, chunk_size=1024):\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=pyaudio.paInt16,\n",
    "                        channels=1,\n",
    "                        rate=sample_rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=chunk_size)\n",
    "    frames = []\n",
    "    print(\"Recording...\")\n",
    "    for i in range(int(sample_rate / chunk_size * duration)):\n",
    "        data = stream.read(chunk_size)\n",
    "        frames.append(data)\n",
    "    print(\"Finished recording.\")\n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    \n",
    "    with wave.open(file_name, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "def convert_to_mp3(input_file, output_file):\n",
    "    audio = AudioSegment.from_wav(input_file)\n",
    "    audio.export(output_file, format=\"mp3\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"input_audio.mp3\"\n",
    "    record_audio(f'Audio/Audio_{datetime.now().strftime(\"%H%M%S\")}.mp3')\n",
    "    print(f\"Audio saved...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioSegment.from_wav('Audio/Audio_172641.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Hello 1 2 3 1 2 3 1 2 3 1 1 2 3\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "whisper_model = WhisperModel(\"Models/faster-whisper-small\")\n",
    "segments, info = whisper_model.transcribe(\"Audio/Audio_181049.mp3\")\n",
    "result=''\n",
    "for segment in segments:\n",
    "    result+=segment.text\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, hello, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 8.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "whisper_model = WhisperModel(\"Models/faster-whisper-small.en\")\n",
    "segments, info = whisper_model.transcribe(\"Audio/Audio_181049.mp3\")\n",
    "result=''\n",
    "for segment in segments:\n",
    "    result+=segment.text\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, hello, hello, one, two, three, one, two, three, one, one, two, three.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "whisper_model = WhisperModel(\"Models/faster-distil-whisper-large-v2\")\n",
    "segments, info = whisper_model.transcribe(\"Audio/Audio_181049.mp3\")\n",
    "result=''\n",
    "for segment in segments:\n",
    "    result+=segment.text\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hello hello 1 2 3 1 2 3 1 2 3 1 1 2 3\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "whisper_model = WhisperModel(\"Models/faster-whisper-medium\")\n",
    "segments, info = whisper_model.transcribe(\"Audio/Audio_181049.mp3\")\n",
    "result=''\n",
    "for segment in segments:\n",
    "    result+=segment.text\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Models/wav2vec2-large-xlsr-53-english were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at Models/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not load the `decoder` for Models/wav2vec2-large-xlsr-53-english. Defaulting to raw CTC. Error: No module named 'kenlm'\n",
      "Try to install `kenlm`: `pip install kenlm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apper ayu a gau ecayu e ayu urtics\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\",model=\"Models/wav2vec2-large-xlsr-53-english\")\n",
    "\n",
    "# Load an audio file\n",
    "audio_file = \"Audio/Audio_181049.mp3\"\n",
    "audio_data, sample_rate = sf.read(audio_file)\n",
    "\n",
    "# Transcribe the audio data\n",
    "transcription = pipe(audio_data)[\"text\"]\n",
    "\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n",
    "\n",
    "transcriptions = model.transcribe(audio_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "demo=gr.Blocks()\n",
    "\n",
    "def transcribe(filepath,a=16000):\n",
    "    if filepath is None:\n",
    "        gr.Warning('No audio found')\n",
    "        return \"\"\n",
    "    segments, info = whisper_model.transcribe(filepath)\n",
    "    result=''\n",
    "    for segment in segments:\n",
    "        result+=segment.text\n",
    "#     result=asr_transcript(filepath)\n",
    "    return result\n",
    "sample_rate_choices = [8000, 16000, 22050, 44100, 48000] \n",
    "mic_transcribe=gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=[\n",
    "        gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "        gr.Dropdown(sample_rate_choices, label=\"Sample Rate\", value=16000)\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Transcription\",\n",
    "                      lines=3),\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "file_transcribe = gr.Interface(\n",
    "    inputs=[\n",
    "        gr.Audio(sources=\"upload\", type=\"filepath\"),\n",
    "        gr.Dropdown(sample_rate_choices, label=\"Sample Rate\", value=16000)\n",
    "    ],\n",
    "    fn=transcribe,\n",
    "    outputs=gr.Textbox(label=\"Transcription\", lines=3),\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "interface_list = [mic_transcribe, file_transfribe]\n",
    "tab_names = [\"Microphone Transcribe\", \"File Transcribe\"]\n",
    "tabbed_interface = gr.TabbedInterface(interface_list, tab_names)\n",
    "\n",
    "# Render the TabbedInterface\n",
    "tabbed_interface.launch(debug=True)\n",
    "# demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "import os\n",
    "\n",
    "def load_file(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if file_extension == '.pdf':\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        loader = UnstructuredFileLoader(file_path)\n",
    "    elif file_extension in ['.txt', '.md']:\n",
    "        loader = TextLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    \n",
    "    data = loader.load()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loader = load_file(\"./Library/Data_Driven_Science_and_Engineering_Machine_Learning_and_Dynamical_Systems.pdf\")\n",
    "# all_text=''.join([i.page_content for i in loader])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "import configparser,base64\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.properties')\n",
    "APIKey = config['AzureCredentials']['APIKey']\n",
    "APIKey=base64.b64decode(APIKey).decode('utf-8')\n",
    "Endpoint = config['AzureCredentials']['Endpoint']\n",
    "Deployment = config['AzureCredentials']['Deployment']\n",
    "version = config['AzureCredentials']['version']\n",
    "EmbeddingDeployment = config['AzureCredentials']['EmbeddingDeployment']\n",
    "text_splitter = SemanticChunker(AzureOpenAIEmbeddings(azure_endpoint=Endpoint,\n",
    "                                                        api_key=APIKey,\n",
    "                                                        deployment=EmbeddingDeployment,\n",
    "                                                        api_version=version), breakpoint_threshold_type=\"percentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = text_splitter.create_documents([all_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import Document\n",
    "from llama_index.core import ServiceContext,VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "llm =AzureOpenAI(\n",
    "    deployment_name=Deployment,\n",
    "    api_key=APIKey,\n",
    "    azure_endpoint=Endpoint,\n",
    "    api_version=version)\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    deployment_name=EmbeddingDeployment,\n",
    "    api_key=APIKey,\n",
    "    azure_endpoint=Endpoint,\n",
    "    api_version=version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document=SimpleDirectoryReader(input_files=['./Library/physics_for_people_in_hurry.pdf']).load_data()\n",
    "document=SimpleDirectoryReader(input_files=['./Library/siddharths_resume.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIVERMA\\AppData\\Local\\Temp\\ipykernel_45732\\3302766004.py:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  Service_Context=ServiceContext.from_defaults(llm=llm,embed_model=embed_model)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Service_Context\u001b[38;5;241m=\u001b[39mServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(llm\u001b[38;5;241m=\u001b[39mllm,embed_model\u001b[38;5;241m=\u001b[39membed_model)\n\u001b[1;32m----> 2\u001b[0m index\u001b[38;5;241m=\u001b[39mVectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(\u001b[43mdocument\u001b[49m,service_context\u001b[38;5;241m=\u001b[39mService_Context,show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m query_engine\u001b[38;5;241m=\u001b[39mindex\u001b[38;5;241m.\u001b[39mas_query_engine()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'document' is not defined"
     ]
    }
   ],
   "source": [
    "Service_Context=ServiceContext.from_defaults(llm=llm,embed_model=embed_model)\n",
    "index=VectorStoreIndex.from_documents(document,service_context=Service_Context,show_progress=True)\n",
    "query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The resume provides information about Siddharth Verma's academic qualifications, technical skills, internships, projects, additional qualifications, and certificates. It includes details about his B.Tech in Computer Science from Graphic Era Hill University Dehradun, his proficiency in programming languages such as Java and C, his experience with tools and frameworks like TensorFlow and Android Studio, and his completion of various courses and certifications related to machine learning and cloud computing. The resume also highlights his experience with internships in Salesforce and CISCO, as well as his involvement in projects such as Twitter Sentiment Analysis and Hand Gesture Recognition.\""
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query('what is the this resume talks about').response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens_eval\n",
    "from llama_index.core import Document\n",
    "document=SimpleDirectoryReader(input_files=['./Library/siddharths_resume.pdf']).load_data()\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in document]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "node_parser=SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key='window',\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIVERMA\\AppData\\Local\\Temp\\ipykernel_15652\\798995337.py:2: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  sentece_context=Service_Context.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "sentece_context=Service_Context.from_defaults(\n",
    "    llm=llm,embed_model=embed_model,node_parser=node_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "sentence_index=VectorStoreIndex.from_documents(\n",
    "    [document],service_context=sentece_context\n",
    ")\n",
    "sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex,StorageContext,load_index_from_storage\n",
    "if not os.path.exists('./sentence_index'):\n",
    "    sentence_index=VectorStoreIndex.from_documents([document],service_context=sentece_context)\n",
    "else:\n",
    "    sentence_index=load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir='./sentence_index'),\n",
    "        service_context=sentece_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "postproc=MetadataReplacementPostProcessor(\n",
    "    target_metadata_key='window'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from copy import deepcopy\n",
    "\n",
    "score_nodes=[NodeWithScore(node=x,score=1.0) for x in nodes]\n",
    "\n",
    "nodes_old=[deepcopy(n) for n in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from copy import deepcopy\n",
    "\n",
    "score_nodes=[NodeWithScore(node=x,score=1.0) for x in nodes]\n",
    "nodes_old=[deepcopy(n) for n in nodes]\n",
    "nodes_old[0].text\n",
    "replaced_nodes = postproc.postprocess_nodes(score_nodes)\n",
    "replaced_nodes[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "# import os\n",
    "# os.environ['CURL_CA_BUNDLE'] = ''\n",
    "# rerank = SentenceTransformerRerank(\n",
    "#     top_n=2, model=\"BAAI/bge-reranker-base\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.schema import TextNode,NodeWithScore\n",
    "query = QueryBundle(\"I want a dog.\")\n",
    "\n",
    "scored_nodes = [\n",
    "    NodeWithScore(node=TextNode(text=\"This is a cat\"), score=0.6),\n",
    "    NodeWithScore(node=TextNode(text=\"This is a dog\"), score=0.4),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranked_nodes = rerank.postprocess_nodes(\n",
    "#     scored_nodes, query_bundle=query\n",
    "# )\n",
    "# print([(x.text, x.score) for x in reranked_nodes])\n",
    "sentence_window_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=6, node_postprocessors=[postproc, rerank]\n",
    ")\n",
    "window_response = sentence_window_engine.query(\n",
    "    \"What are the keys to building a career in AI?\"\n",
    ")\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "display_response(window_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "from llama_index.core import SimpleDirectoryReader,Document\n",
    "# document=SimpleDirectoryReader(input_files=['./Library/physics_for_people_in_hurry.pdf']).load_data()\n",
    "# create the hierarchical node parser w/ default settings\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128]\n",
    ")\n",
    "# document=Document(text='\\n\\n'.join([i.text for i in document]))\n",
    "# nodes = node_parser.get_nodes_from_documents([document])\n",
    "# leaf_nodes=get_leaf_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import get_leaf_nodes,get_deeper_nodes,get_root_nodes\n",
    "# leaf_nodes=get_leaf_nodes(nodes)\n",
    "# depper_nodes=get_deeper_nodes(nodes)\n",
    "# root_nodes=get_root_nodes(nodes)\n",
    "# nodes_by_id = {node.node_id: node for node in nodes}\n",
    "# parent_node = nodes_by_id[leaf_nodes[30].parent_node.node_id]\n",
    "# print(parent_node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIVERMA\\AppData\\Local\\Temp\\ipykernel_45732\\3921966514.py:17: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  Service_Context=ServiceContext.from_defaults(llm=llm,embed_model=embed_model,node_parser=node_parser)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import PromptTemplate\n",
    "llm =AzureOpenAI(\n",
    "    deployment_name=Deployment,\n",
    "    api_key=APIKey,\n",
    "    azure_endpoint=Endpoint,\n",
    "    api_version=version\n",
    ")\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    deployment_name=EmbeddingDeployment,\n",
    "    api_key=APIKey,\n",
    "    azure_endpoint=Endpoint,\n",
    "    api_version=version,\n",
    ")\n",
    "Service_Context=ServiceContext.from_defaults(llm=llm,embed_model=embed_model,node_parser=node_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\"\"\"You are an AI language model assistant. Your task is to generate Five\n",
    "    different versions of the given user question to retrieve relevant documents from a vector\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Provide these alternative questions seperated by newlines.\n",
    "    Original question: {question}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.complete(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser,get_leaf_nodes,get_deeper_nodes,get_root_nodes\n",
    "from llama_index.core import SimpleDirectoryReader,Document\n",
    "import openai\n",
    "import configparser,base64\n",
    "# ____________________________________________ reading config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.properties')\n",
    "APIKey = config['AzureCredentials']['APIKey']\n",
    "APIKey=base64.b64decode(APIKey).decode('utf-8')\n",
    "Endpoint = config['AzureCredentials']['Endpoint']\n",
    "Deployment = config['AzureCredentials']['Deployment']\n",
    "version = config['AzureCredentials']['version']\n",
    "EmbeddingDeployment = config['AzureCredentials']['EmbeddingDeployment']\n",
    "\n",
    "# ____________________________________________ os enviourment \n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = Endpoint\n",
    "os.environ[\"OPENAI_API_KEY\"] = APIKey\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = version\n",
    "os.environ[\"azure_endpoint\"] = Endpoint\n",
    "os.environ[\"azure_endpoint\"] = Endpoint\n",
    "def create_or_load_ko_dump(file_name):\n",
    "    dump_path=f\"knowledge_dump/{file_name.split('.')[0]}\"\n",
    "    if not os.path.exists(f\"knowledge_dump/{file_name}\"):\n",
    "\n",
    "#   ____________________________________________________________________ build node parser \n",
    "    \n",
    "        document=SimpleDirectoryReader(input_files=[f'./Library/{file_name}']).load_data()\n",
    "        # create the hierarchical node parser w/ default settings\n",
    "        node_parser = HierarchicalNodeParser.from_defaults(\n",
    "            chunk_sizes=[2048, 512, 128]\n",
    "        )\n",
    "        document=Document(text='\\n\\n'.join([i.text for i in document]))\n",
    "        nodes = node_parser.get_nodes_from_documents([document])\n",
    "        leaf_nodes=get_leaf_nodes(nodes)\n",
    "\n",
    "\n",
    "#   _______________________________________________________________ buidling service context \n",
    "        llm =AzureOpenAI(\n",
    "            deployment_name=Deployment,\n",
    "            api_key=APIKey,\n",
    "            azure_endpoint=Endpoint,\n",
    "            api_version=version\n",
    "        )\n",
    "        embed_model = AzureOpenAIEmbedding(\n",
    "            deployment_name=EmbeddingDeployment,\n",
    "            api_key=APIKey,\n",
    "            azure_endpoint=Endpoint,\n",
    "            api_version=version,\n",
    "        )\n",
    "        Service_Context=ServiceContext.from_defaults(llm=llm,embed_model=embed_model,node_parser=node_parser)   \n",
    "\n",
    "#   ____________________________________________________________ storage context \n",
    "        storage_context = StorageContext.from_defaults()\n",
    "        storage_context.docstore.add_documents(nodes)\n",
    "        automerging_index = VectorStoreIndex(\n",
    "                leaf_nodes,\n",
    "                storage_context=storage_context,\n",
    "                service_context=Service_Context\n",
    "            )\n",
    "        automerging_index.storage_context.persist(persist_dir=dump_path)\n",
    "    else:\n",
    "        automerging_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=dump_path),\n",
    "            service_context=Service_Context\n",
    "        )\n",
    "        \n",
    "    return automerging_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIVERMA\\AppData\\Local\\Temp\\ipykernel_45732\\1051250160.py:57: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  Service_Context=ServiceContext.from_defaults(llm=llm,embed_model=embed_model,node_parser=node_parser)\n"
     ]
    }
   ],
   "source": [
    "automerging_index=create_or_load_ko_dump('/WHO.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank,EmbeddingRecencyPostprocessor\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = APIKey\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "automerging_retriever = automerging_index.as_retriever(\n",
    "    similarity_top_k=12\n",
    ")\n",
    "\n",
    "retriever = AutoMergingRetriever(\n",
    "    automerging_retriever, \n",
    "    automerging_index.storage_context, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "rerank = EmbeddingRecencyPostprocessor(top_n=6, model=embed_model)\n",
    "\n",
    "auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "    automerging_retriever ,node_postprocessors=[rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# auto_merging_response = auto_merging_engine.query(\n",
    "#     \"what is this documnet about\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is most interesting about this essay?\"\n",
    "# query_engine = index.as_query_engine()\n",
    "# query_engine.query(query).response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_merging_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index.core import (\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "from llama_index.core.node_parser import get_leaf_nodes\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor,PrevNextNodePostprocessor,KeywordNodePostprocessor\n",
    "from llama_index.core.data_structs import Node\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "\n",
    "def build_automerging_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index\",\n",
    "    chunk_sizes=None,\n",
    "):\n",
    "    chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "    merging_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes, storage_context=storage_context, service_context=merging_context\n",
    "        )\n",
    "        automerging_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        automerging_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=merging_context,\n",
    "        )\n",
    "    return automerging_index\n",
    "\n",
    "\n",
    "def get_automerging_query_engine(\n",
    "    automerging_index,\n",
    "    similarity_top_k=12,\n",
    "    response_mode='refine'\n",
    "):\n",
    "    \n",
    "    retriever = VectorIndexRetriever(\n",
    "    index=automerging_index,\n",
    "    similarity_top_k=similarity_top_k,\n",
    "    )\n",
    "    \n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        llm=llm,\n",
    "        response_mode=response_mode,\n",
    "        streaming=True,\n",
    "        # structured_answer_filtering=True\n",
    "    )\n",
    "    '''\n",
    "            default: \"create and refine\" an answer by sequentially going through each retrieved Node; \n",
    "                      This makes a separate LLM call per Node. Good for more detailed answers.\n",
    "            \n",
    "            compact: \"compact\" the prompt during each LLM call by stuffing as many Node text chunks \n",
    "                      that can fit within the maximum prompt size.If there are too many chunks to stuff in one prompt, \n",
    "                      \"create and refine\" an answer by going through multiple prompts.\n",
    "            \n",
    "            tree_summarize: Given a set of Node objects and the query, recursively construct a tree and return the root \n",
    "                            node as the response. Good for summarization purposes.\n",
    "            \n",
    "            no_text: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n",
    "                     without actually sending them. Then can be inspected by checking response.source_nodes. The response object \n",
    "                     is covered in more detail in Section 5.\n",
    "            \n",
    "            accumulate: Given a set of Node objects and the query, apply the query to each Node text chunk while accumulating \n",
    "                        the responses into an array. Returns a concatenated string of all responses. Good for when you need to \n",
    "                        run the same query separately against each text chunk.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # _____________________________ KeywordNodePostprocessor : Filters nodes by required_keywords and exclude_keywords - not good\n",
    "    # node_postprocessors = [\n",
    "    #     KeywordNodePostprocessor(\n",
    "    #         required_keywords=[\"Combinator\"], exclude_keywords=[\"Italy\"]\n",
    "    #     )\n",
    "    # ]\n",
    "\n",
    "\n",
    "    # ______________________________  SimilarityPostprocessor :  filters nodes by setting a threshold on the similarity score\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever,\n",
    "        node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.4)],\n",
    "        response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    \n",
    "    return query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from llama_index.llms.azure_openai import OpenAI\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# index = build_automerging_index(\n",
    "#     [document],\n",
    "#     llm=llm,\n",
    "#     save_dir=\"./merging_index\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_automerging_query_engine(automerging_index, similarity_top_k=12,response_mode='tree_summarize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine.query(\n",
    "#     \"how dark matter work against dark energy\"\n",
    "# ).response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in query_engine.query('what is coreo').response_gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=AzureOpenAIEmbedding(model_name='text-embedding-ada-002', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002C818FBAC10>, additional_kwargs={}, api_key='d4ac1064b3cd45afb5883ca060db4475', api_base='https://api.openai.com/v1', api_version='2023-05-15', max_retries=10, timeout=60.0, default_headers=None, reuse_client=True, dimensions=None, azure_endpoint='https://genai-openai-na.openai.azure.com/', azure_deployment='NA-Text-EmbeddingADA-002'), transformations=[HierarchicalNodeParser(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002C818FD9DD0>, id_func=None, chunk_sizes=[2048, 512, 128], node_parser_ids=['chunk_size_2048', 'chunk_size_512', 'chunk_size_128'], node_parser_map={'chunk_size_2048': SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002C818FD9DD0>, id_func=<function default_id_func at 0x000002C80C0DC180>, chunk_size=2048, chunk_overlap=20, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?'), 'chunk_size_512': SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002C818FD9DD0>, id_func=<function default_id_func at 0x000002C80C0DC180>, chunk_size=512, chunk_overlap=20, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?'), 'chunk_size_128': SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002C818FD9DD0>, id_func=<function default_id_func at 0x000002C80C0DC180>, chunk_size=128, chunk_overlap=20, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')})], llama_logger=<llama_index.core.service_context_elements.llama_logger.LlamaLogger object at 0x000002C819018690>, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002C818FBAC10>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=automerging_index,\n",
    "    similarity_top_k=12,\n",
    "    )\n",
    "# vector_index: BaseIndex = VectorStoreIndex.from_documents(\n",
    "#     docs,\n",
    "#     service_context=service_context, \n",
    "#     show_progress=True,\n",
    "# )\n",
    "retriever.get_service_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueriesRetriever():\n",
    "    def __init__(self):\n",
    "        self.template = PromptTemplate(\"\"\"You are an AI language model assistant. Your task is to generate four\n",
    "    different versions of the given user question to retrieve relevant documents from a vector\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Provide these alternative questions seperated by newlines.\n",
    "    Original question: {question}\"\"\")\n",
    "        \n",
    "        self.model = llm\n",
    "    \n",
    "    def gen_queries(self, query)->str:\n",
    "        prompt = self.template.format(question=query)\n",
    "        res = self.model.complete(prompt)\n",
    "        return '\\n'.join(res.text.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Could you provide a brief overview of the contents of this paper?\\n2. Can you give me a synopsis of the main points covered in this paper?\\n3. Would you be able to condense the information presented in this paper into a summary?\\n4. Is it possible for you to provide a summary of the key findings and conclusions of this paper?'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "final_res = MultiQueriesRetriever().gen_queries('can you summarize this paper')\n",
    "final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Could you provide a brief overview of the contents of this paper?\\n2. Can you give me a synopsis of what this paper is about?\\n3. Would you be able to condense the main points of this paper for me?\\n4. Is it possible for you to provide a summary of the key findings in this paper?'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n'.join(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\"\"\"You are an AI language model assistant. Your task is to generate Five\n",
    "    different versions of the given user question to retrieve relevant documents from a vector\n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Provide these alternative questions seperated by newlines.\n",
    "    Original question: {question}\"\"\")\n",
    "prompt = template.format(question='why dark matter and dark enrgy is activate area of research')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='1. What makes dark matter and dark energy such active areas of research?\\n2. Why are scientists so interested in studying dark matter and dark energy?\\n3. What drives the ongoing research on dark matter and dark energy?\\n4. What are the reasons behind the intense focus on dark matter and dark energy in scientific research?\\n5. What motivates researchers to investigate dark matter and dark energy?', additional_kwargs={}, raw={'id': 'chatcmpl-99nk6HVEQoZwEmuFJgvOGtK0flhIK', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1. What makes dark matter and dark energy such active areas of research?\\n2. Why are scientists so interested in studying dark matter and dark energy?\\n3. What drives the ongoing research on dark matter and dark energy?\\n4. What are the reasons behind the intense focus on dark matter and dark energy in scientific research?\\n5. What motivates researchers to investigate dark matter and dark energy?', role='assistant', function_call=None, tool_calls=None))], 'created': 1712123190, 'model': 'gpt-35-turbo', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=78, prompt_tokens=100, total_tokens=178)}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.complete(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 text chunks after repacking\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "# NOTE: we add an extra tone_name variable here\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Please also write the answer in the tone of {tone_name}.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)\n",
    "\n",
    "# initialize response synthesizer\n",
    "summarizer = TreeSummarize(llm=llm,verbose=True, summary_template=qa_prompt)\n",
    "\n",
    "# get response\n",
    "response = summarizer.get_response(\n",
    "    \"how dark matter work against dark energy\", [i.text for i in query_engine.retrieve('how dark matter work against dark energy')], tone_name=\"a pirate voice\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index\",\n",
    "):\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            documents, service_context=sentence_context\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index, similarity_top_k=6, rerank_top_n=2\n",
    "):\n",
    "    # define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "index = build_sentence_window_index(\n",
    "    [document],\n",
    "    llm=AzureOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    save_dir=\"./sentence_index\",\n",
    ")\n",
    "query_engine = get_sentence_window_query_engine(index, similarity_top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "from llama_index.core.node_parser import get_leaf_nodes\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "def build_automerging_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index\",\n",
    "    chunk_sizes=None,\n",
    "):\n",
    "    chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "    merging_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes, storage_context=storage_context, service_context=merging_context\n",
    "        )\n",
    "        automerging_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        automerging_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=merging_context,\n",
    "        )\n",
    "    return automerging_index\n",
    "\n",
    "\n",
    "def get_automerging_query_engine(\n",
    "    automerging_index,\n",
    "    similarity_top_k=12,\n",
    "    rerank_top_n=6,\n",
    "):\n",
    "    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    retriever = AutoMergingRetriever(\n",
    "        base_retriever, automerging_index.storage_context, verbose=True\n",
    "    )\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "    auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever, node_postprocessors=[rerank]\n",
    "    )\n",
    "    return auto_merging_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text=\"hello how are you? i am fine\"\n",
    "# nodes=node_parser.get_nodes_from_documents([Document(text=text)])\n",
    "# print(nodes[1].metadata['window'])\n",
    "# text = \"hello. foo bar. cat dog. mouse\"\n",
    "# nodes = node_parser.get_nodes_from_documents([Document(text=text)])\n",
    "# print(nodes[1].metadata['window'])\n",
    "\n",
    "# from llama_index.core.schema import NodeWithScore\n",
    "# from copy import deepcopy\n",
    "\n",
    "# score_nodes=[NodeWithScore(node=x,score=1.0) for x in nodes]\n",
    "# nodes_old=[deepcopy(n) for n in nodes]\n",
    "# nodes_old[0].text\n",
    "# replaced_nodes = postproc.postprocess_nodes(score_nodes)\n",
    "# replaced_nodes[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = WhisperModel(\"guillaumekln/faster-whisper-medium\",download_root='Models/faster-whisper-medium')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiate the pipeline\n",
    "# from pyannote.audio import Pipeline\n",
    "# pipeline = Pipeline.from_pretrained(\n",
    "#   \"pyannote/speaker-diarization-3.1\",\n",
    "#   use_auth_token=\"HUGGINGFACE_ACCESS_TOKEN_GOES_HERE\")\n",
    "\n",
    "\n",
    "# # run the pipeline on an audio file\n",
    "# diarization = pipeline(\"audio.wav\")\n",
    "\n",
    "# # dump the diarization output to disk using RTTM format\n",
    "# with open(\"audio.rttm\", \"w\") as rttm:\n",
    "#     diarization.write_rttm(rttm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import librosa\n",
    "# load model and tokenizer\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"Models/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"Models/wav2vec2-base-960h\")\n",
    "tokenizer=Wav2Vec2Tokenizer.from_pretrained(\"Models/wav2vec2-base-960h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_casing(input_sentence):\n",
    "    sentences = nltk.sent_tokenize(input_sentence)\n",
    "    return (' '.join([s.replace(s[0],s[0].capitalize(),1) for s in sentences]))\n",
    "\n",
    "def load_data(input_file):\n",
    "    #reading the file\n",
    "    speech, sample_rate = librosa.load(input_file)\n",
    "    #make it 1-D\n",
    "    if len(speech.shape) > 1:\n",
    "        speech = speech[:,0] + speech[:,1]\n",
    "    #Resampling the audio at 16KHz\n",
    "    if sample_rate !=16000:\n",
    "        speech = librosa.resample(speech, orig_sr=sample_rate,target_sr=16000)\n",
    "    return speech\n",
    "\n",
    "def asr_transcript(input_file):\n",
    "    speech = load_data(input_file)\n",
    "    #Tokenize\n",
    "    input_values = tokenizer(speech, return_tensors=\"pt\").input_values\n",
    "    #Take logits\n",
    "    logits = model(input_values).logits\n",
    "    #Take argmax\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    #Get the words from predicted word ids\n",
    "    transcription = tokenizer.decode(predicted_ids[0])\n",
    "    #Correcting the letter casing\n",
    "    transcription = correct_casing(transcription.lower())\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=asr_transcript('Audio/Audio_222237.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_excel('all_combined_question_answers.xlsx')\n",
    "df.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Knowledge graph implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (SimpleDirectoryReader,\n",
    "                         ServiceContext,\n",
    "                         KnowledgeGraphIndex)\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"C:/Users/SIVERMA/Documents/Experimenting/LETSDISCUSS/Library/high_jacobi_struct.pdf\"]).load_data()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIVERMA\\AppData\\Local\\Temp\\ipykernel_45732\\1355109530.py:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=256,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = KnowledgeGraphIndex.from_documents( documents=documents,\n",
    "                                           max_triplets_per_chunk=3,\n",
    "                                           service_context=service_context,\n",
    "                                           storage_context=storage_context,\n",
    "                                          include_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(include_text=True,\n",
    "                                     response_mode =\"tree_summarize\",\n",
    "                                     embedding_mode=\"hybrid\",\n",
    "                                     similarity_top_k=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ans(query):\n",
    "    message_template =f\"\"\"<|system|>Please check if the following pieces of context has any mention of the  keywords provided in the Question.If not then don't know the answer, just say that you don't know.Stop there.Please donot try to make up an answer.</s>\n",
    "    <|user|>\n",
    "    Question: {query}\n",
    "    Helpful Answer:\n",
    "    </s>\"\"\"\n",
    "    return query_engine.query(message_template).response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jacobi sigma models are two-dimensional topological non-linear field theories associated with Jacobi structures, which are a generalization of Poisson structures. They are also considered as topological non-linear gauge field theories. The models have been reviewed in various papers, including one by Francesco Bascone, Franco Pezzella, and Patrizia Vitale. The twisted version of the Jacobi sigma model includes a Wess-Zumino term to accommodate twisted Jacobi backgrounds as target spaces. The models have been studied on various manifolds, including the five-dimensional sphere S5.'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ans('what is jacobi sigma model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = AutoMergingRetriever(retriever ,StorageContext.from_defaults(persist_dir='./knowledge_dump/Thermodynamic_dissipation_does') ,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "index,storage=create_or_load_ko_dump('darl_photon.pdf')\n",
    "query_engine=get_automerging_query_engine(index,storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONVO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
